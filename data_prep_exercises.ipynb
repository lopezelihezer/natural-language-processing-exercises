{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eca1634-b452-4054-94c0-00b7ae288d69",
   "metadata": {},
   "source": [
    "Data Preparation Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b009af-5e60-4f55-82b2-0bf62b41e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840f696-d0c1-42a6-9127-e23ffeed1bf7",
   "metadata": {},
   "source": [
    "(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0e9630-477b-48a2-8e19-c0c7b5e3ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_clean(string):\n",
    "    string = string.lower() # lowercase everything\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8') # normalize unicode characters\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string) # replaces anything that is not a letter, number, whitespace or a single quote.\n",
    "    return string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81080407-4b49-4cb5-9000-1f6cdf2f2e2d",
   "metadata": {},
   "source": [
    "(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976c73a0-5fbb-46e5-96c5-0c51888b0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    # Create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    # Use the tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True) # False returns list of tokens, True returns entire string\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82417c-eff4-4d65-82cd-f2517f463a5f",
   "metadata": {},
   "source": [
    "(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4143a6-b73d-4f75-8a99-0192db2ccac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    ps = nltk.porter.PorterStemmer() # Create porter stemmer.\n",
    "    # Apply the stemmer to each word in our string.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again; assign to a variable to save changes\n",
    "    string_stemmed = ' '.join(stems)\n",
    "    return string_stemmed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f617c95-6284-43f0-a5d9-0e47144db3cb",
   "metadata": {},
   "source": [
    "(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9c3bac-4c23-4e19-84eb-15b20f4ba261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    # Create the Lemmatizer.\n",
    "\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "\n",
    "    # Join our list of words into a string again; assign to a variable to save changes.\n",
    "    # Assumes they are all nouns\n",
    "\n",
    "    string_lemmatized = ' '.join(lemmas)\n",
    "    return string_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be37d5-0f16-472c-bbcb-ffcae3aacec9",
   "metadata": {},
   "source": [
    "(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2514c374-9799-4546-b93f-4e8301ff3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words, exclude_words): #extra_words is list of words to include, exclude_words is list of words to exclude\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # you can add or remove from stopword list \n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "    # Split words in lemmatized article.\n",
    "\n",
    "    words = lemmatize(string).split()\n",
    "\n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "\n",
    "    # Join words in the list back into strings; assign to a variable to keep changes.\n",
    "\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "    return article_without_stopwords\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a660fc-30c9-4842-8f5b-2c004566a027",
   "metadata": {},
   "source": [
    "(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6d61f-8576-4129-a6fc-d051a6e01d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
